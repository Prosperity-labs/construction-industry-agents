#!/usr/bin/env python3
"""
Comprehensive Test Runner with Visual Real-time Communication
Easy-to-repeat testing framework for the Construction Industry Agents system
"""

import os
import sys
import time
import json
import subprocess
from pathlib import Path
from typing import Dict, List, Optional
from datetime import datetime
import threading
import argparse

class VisualTestRunner:
    """Visual test runner with real-time communication and easy repeatability"""
    
    def __init__(self):
        self.test_session_id = f"TEST_{int(time.time())}"
        self.results = {"session_id": self.test_session_id, "tests": [], "summary": {}}
        self.start_time = time.time()
        
    def clear_screen(self):
        """Clear terminal screen"""
        os.system('clear' if os.name == 'posix' else 'cls')
    
    def print_header(self, title: str):
        """Print a styled header"""
        self.clear_screen()
        print("‚ïî" + "‚ïê" * 78 + "‚ïó")
        print(f"‚ïë{title.center(78)}‚ïë")
        print("‚ï†" + "‚ïê" * 78 + "‚ï£")
        print(f"‚ïë Session: {self.test_session_id:<30} ‚îÇ Time: {datetime.now().strftime('%H:%M:%S'):<15} ‚ïë")
        print("‚ïö" + "‚ïê" * 78 + "‚ïù")
        print()
    
    def print_progress_bar(self, current: int, total: int, test_name: str = ""):
        """Print visual progress bar"""
        if total == 0:
            return
        percentage = (current / total) * 100
        bar_length = 50
        filled_length = int(bar_length * current // total)
        
        bar = "‚ñà" * filled_length + "‚ñë" * (bar_length - filled_length)
        print(f"üîÑ {test_name}: [{bar}] {percentage:.1f}% ({current}/{total})")
    
    def run_test_with_visual_feedback(self, test_name: str, test_func, *args, **kwargs):
        """Run a test with visual feedback"""
        print(f"\nüß™ Starting: {test_name}")
        print("‚îÄ" * 60)
        
        start_time = time.time()
        
        try:
            # Show "running" indicator
            print("üîÑ Executing test...")
            
            # Run the test
            result = test_func(*args, **kwargs)
            
            duration = time.time() - start_time
            
            # Show success
            print(f"‚úÖ PASSED in {duration:.1f}s")
            if isinstance(result, dict) and "details" in result:
                for detail in result["details"]:
                    print(f"   ‚îî‚îÄ {detail}")
            elif isinstance(result, str):
                print(f"   ‚îî‚îÄ {result}")
            
            # Store result
            self.results["tests"].append({
                "name": test_name,
                "status": "PASSED",
                "duration": duration,
                "details": result
            })
            
            return True
            
        except Exception as e:
            duration = time.time() - start_time
            print(f"‚ùå FAILED in {duration:.1f}s")
            print(f"   ‚îî‚îÄ Error: {str(e)}")
            
            # Store result
            self.results["tests"].append({
                "name": test_name,
                "status": "FAILED", 
                "duration": duration,
                "error": str(e)
            })
            
            return False
    
    def test_system_components(self):
        """Test all system components individually"""
        components = [
            ("Excel Parser Agent", "python excel_parser_agent.py --no-llm"),
            ("Supplier Mapping Agent", "python supplier_mapping_agent.py"),
            ("Communication Agent", "python communication_agent.py"),
            ("Response Parser Agent", "python response_parser_agent.py"),
            ("Quote Calculator Agent", "python quote_calculator_agent.py"),
            ("Document Generator Agent", "python document_generator_agent.py")
        ]
        
        results = []
        for i, (name, command) in enumerate(components, 1):
            self.print_progress_bar(i, len(components), "Component Testing")
            
            try:
                result = subprocess.run(
                    command.split(), 
                    capture_output=True, 
                    text=True, 
                    timeout=30,
                    cwd=os.getcwd()
                )
                
                if result.returncode == 0:
                    results.append(f"‚úÖ {name}")
                    print(f"   ‚úÖ {name}: OK")
                else:
                    results.append(f"‚ùå {name}: {result.stderr[:50]}...")
                    print(f"   ‚ùå {name}: FAILED")
                    
            except subprocess.TimeoutExpired:
                results.append(f"‚è∞ {name}: Timeout")
                print(f"   ‚è∞ {name}: TIMEOUT")
            except Exception as e:
                results.append(f"‚ùå {name}: {str(e)[:50]}...")
                print(f"   ‚ùå {name}: ERROR")
            
            time.sleep(0.5)  # Visual pause
        
        return {"component_results": results, "details": results}
    
    def test_workflow_integration(self):
        """Test complete workflow integration"""
        print("üîÑ Testing complete workflow integration...")
        
        try:
            # Test with demo file
            result = subprocess.run([
                sys.executable, "workflow_orchestrator.py"
            ], capture_output=True, text=True, timeout=120)
            
            if result.returncode != 0:
                raise Exception(f"Workflow failed: {result.stderr}")
            
            # Check outputs
            output_dir = Path("complete_workflow_output")
            if not output_dir.exists():
                raise Exception("Output directory not created")
            
            files = list(output_dir.glob("*.xlsx")) + list(output_dir.glob("*.json"))
            
            return {
                "status": "SUCCESS",
                "files_generated": len(files),
                "details": [
                    f"Generated {len(files)} output files",
                    f"Workflow completed successfully",
                    f"Output directory: {output_dir}"
                ]
            }
            
        except Exception as e:
            raise Exception(f"Workflow integration test failed: {str(e)}")
    
    def test_visual_workflow(self):
        """Test visual workflow monitor"""
        print("üîÑ Testing visual workflow monitor...")
        
        try:
            # Test visual monitor with timeout
            result = subprocess.run([
                sys.executable, "visual_workflow_monitor.py"
            ], capture_output=True, text=True, timeout=60)
            
            if result.returncode != 0:
                raise Exception(f"Visual workflow failed: {result.stderr}")
            
            # Check if visual output directory was created
            visual_output = Path("visual_workflow_output")
            if not visual_output.exists():
                raise Exception("Visual workflow output not created")
            
            files = list(visual_output.glob("*.xlsx"))
            
            return {
                "status": "SUCCESS",
                "visual_files": len(files),
                "details": [
                    f"Visual workflow completed",
                    f"Generated {len(files)} visual output files",
                    "Real-time display worked correctly"
                ]
            }
            
        except subprocess.TimeoutExpired:
            return {
                "status": "SUCCESS", 
                "details": ["Visual workflow timeout (expected for demo)"]
            }
        except Exception as e:
            raise Exception(f"Visual workflow test failed: {str(e)}")
    
    def test_excel_file_processing(self):
        """Test processing of different Excel files"""
        print("üîÑ Testing Excel file processing...")
        
        test_files = list(Path("tests/input").glob("*.xlsx"))
        if not test_files:
            return {"details": ["No test Excel files found - skipped"]}
        
        results = []
        for i, test_file in enumerate(test_files[:3], 1):  # Test first 3 files
            self.print_progress_bar(i, min(3, len(test_files)), "Excel Processing")
            
            try:
                result = subprocess.run([
                    sys.executable, "excel_parser_agent.py", str(test_file), "--no-llm"
                ], capture_output=True, text=True, timeout=30)
                
                if result.returncode == 0:
                    results.append(f"‚úÖ {test_file.name}")
                else:
                    results.append(f"‚ùå {test_file.name}")
                    
            except Exception:
                results.append(f"‚ùå {test_file.name}: Error")
            
            time.sleep(0.3)
        
        return {"excel_results": results, "details": results}
    
    def test_performance_benchmark(self):
        """Run performance benchmark"""
        print("üîÑ Running performance benchmark...")
        
        start_time = time.time()
        
        try:
            # Run workflow multiple times and measure
            times = []
            for i in range(3):
                self.print_progress_bar(i + 1, 3, "Performance Test")
                
                run_start = time.time()
                result = subprocess.run([
                    sys.executable, "workflow_orchestrator.py"
                ], capture_output=True, text=True, timeout=60)
                
                if result.returncode == 0:
                    times.append(time.time() - run_start)
                
                time.sleep(1)
            
            if times:
                avg_time = sum(times) / len(times)
                return {
                    "average_time": avg_time,
                    "details": [
                        f"Average execution time: {avg_time:.1f}s",
                        f"Fastest run: {min(times):.1f}s",
                        f"Slowest run: {max(times):.1f}s",
                        f"Performance rating: {'üü¢ Excellent' if avg_time < 10 else 'üü° Good' if avg_time < 30 else 'üî¥ Needs optimization'}"
                    ]
                }
            else:
                raise Exception("No successful performance runs")
                
        except Exception as e:
            raise Exception(f"Performance test failed: {str(e)}")
    
    def save_test_results(self):
        """Save test results to file"""
        self.results["summary"] = {
            "total_tests": len(self.results["tests"]),
            "passed": sum(1 for t in self.results["tests"] if t["status"] == "PASSED"),
            "failed": sum(1 for t in self.results["tests"] if t["status"] == "FAILED"),
            "total_duration": time.time() - self.start_time,
            "timestamp": datetime.now().isoformat()
        }
        
        # Save to file
        results_file = Path(f"test_results_{self.test_session_id}.json")
        with open(results_file, 'w') as f:
            json.dump(self.results, f, indent=2)
        
        return results_file
    
    def print_final_summary(self):
        """Print final test summary"""
        summary = self.results["summary"]
        
        self.clear_screen()
        print("‚ïî" + "‚ïê" * 78 + "‚ïó")
        print("‚ïë" + "üéØ TEST SESSION COMPLETED".center(78) + "‚ïë")
        print("‚ï†" + "‚ïê" * 78 + "‚ï£")
        
        # Overall status
        if summary["failed"] == 0:
            status = "üü¢ ALL TESTS PASSED"
            color = "green"
        else:
            status = f"üî¥ {summary['failed']} TESTS FAILED"
            color = "red"
        
        print(f"‚ïë Status: {status:<35} ‚îÇ Duration: {summary['total_duration']:.1f}s" + " " * 15 + "‚ïë")
        print(f"‚ïë Passed: {summary['passed']:<8} Failed: {summary['failed']:<8} Total: {summary['total_tests']:<8}" + " " * 25 + "‚ïë")
        print("‚ï†" + "‚ïê" * 78 + "‚ï£")
        
        # Test details
        print("‚ïë Test Results:" + " " * 63 + "‚ïë")
        for test in self.results["tests"]:
            status_icon = "‚úÖ" if test["status"] == "PASSED" else "‚ùå"
            name = test["name"][:45]
            duration = test["duration"]
            print(f"‚ïë   {status_icon} {name:<45} ‚îÇ {duration:>6.1f}s" + " " * 8 + "‚ïë")
        
        print("‚ïö" + "‚ïê" * 78 + "‚ïù")
        
        # Recommendations
        print(f"\nüí° RECOMMENDATIONS:")
        if summary["failed"] == 0:
            print("  üöÄ System is ready for production deployment!")
            print("  üìä Consider setting up continuous integration")
            print("  üîÑ Run these tests regularly to ensure system health")
        else:
            print("  üîß Fix failing tests before deployment")
            print("  üìù Check error logs for detailed failure information")
            print("  üß™ Re-run tests after fixes")
        
        print(f"\nüìÅ Detailed results saved to: test_results_{self.test_session_id}.json")
    
    def run_full_test_suite(self):
        """Run the complete test suite with visual feedback"""
        self.print_header("üß™ CONSTRUCTION INDUSTRY AGENTS - TEST SUITE")
        
        print("üéØ Running comprehensive test suite with visual feedback...")
        print("üìä This will test all components, workflows, and performance")
        print()
        input("Press Enter to start testing...")
        
        # Define all tests
        tests = [
            ("System Components", self.test_system_components),
            ("Workflow Integration", self.test_workflow_integration),
            ("Visual Workflow", self.test_visual_workflow),
            ("Excel Processing", self.test_excel_file_processing),
            ("Performance Benchmark", self.test_performance_benchmark)
        ]
        
        # Run each test with visual feedback
        for i, (test_name, test_func) in enumerate(tests, 1):
            self.print_header(f"üß™ TEST {i}/{len(tests)}: {test_name.upper()}")
            
            success = self.run_test_with_visual_feedback(test_name, test_func)
            
            print(f"\n‚è±Ô∏è  Test completed. Moving to next test in 2 seconds...")
            time.sleep(2)
        
        # Save results and show summary
        results_file = self.save_test_results()
        self.print_final_summary()
        
        return self.results

def create_test_shortcuts():
    """Create easy-to-use test shortcuts"""
    shortcuts = {
        "quick": "Quick smoke test (2 minutes)",
        "full": "Complete test suite (10 minutes)", 
        "visual": "Visual workflow test only",
        "performance": "Performance benchmark only",
        "components": "Individual component tests only"
    }
    
    return shortcuts

def main():
    """Main test runner function"""
    parser = argparse.ArgumentParser(description="Construction Industry Agents Test Runner")
    parser.add_argument("test_type", nargs="?", default="full", 
                       choices=["quick", "full", "visual", "performance", "components"],
                       help="Type of test to run")
    parser.add_argument("--no-visual", action="store_true", help="Disable visual feedback")
    
    args = parser.parse_args()
    
    print("üèóÔ∏è Construction Industry Agents - Test Runner")
    print("=" * 60)
    
    if len(sys.argv) == 1:
        # Interactive mode
        shortcuts = create_test_shortcuts()
        print("Available test options:")
        for key, description in shortcuts.items():
            print(f"  {key}: {description}")
        
        choice = input("\nSelect test type (or press Enter for 'full'): ").strip() or "full"
        args.test_type = choice if choice in shortcuts else "full"
    
    # Create test runner
    runner = VisualTestRunner()
    
    # Run selected test type
    if args.test_type == "quick":
        print("üöÄ Running quick smoke test...")
        runner.run_test_with_visual_feedback("Quick Smoke Test", runner.test_workflow_integration)
        
    elif args.test_type == "visual":
        print("üé® Running visual workflow test...")
        runner.run_test_with_visual_feedback("Visual Workflow", runner.test_visual_workflow)
        
    elif args.test_type == "performance":
        print("‚ö° Running performance benchmark...")
        runner.run_test_with_visual_feedback("Performance Benchmark", runner.test_performance_benchmark)
        
    elif args.test_type == "components":
        print("üîß Running component tests...")
        runner.run_test_with_visual_feedback("System Components", runner.test_system_components)
        
    else:  # full
        print("üéØ Running complete test suite...")
        runner.run_full_test_suite()
    
    # Save results
    results_file = runner.save_test_results()
    print(f"\n‚úÖ Testing completed! Results saved to: {results_file}")

if __name__ == "__main__":
    main()